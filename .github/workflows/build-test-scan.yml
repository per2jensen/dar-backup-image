# .github/workflows/ci.yml
name: CI — build, test, scan
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

  workflow_dispatch: {}

permissions:
  contents: read
  packages: write         # for GHCR push/pull
  security-events: write  # for SARIF upload

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: ghcr.io/${{ github.repository_owner }}
  IMAGE_NAME: dar-backup
  CI_TAG: ci-${{ github.sha }}

jobs:
  build:
    runs-on: ubuntu-24.04
    env:
      SUMMARY_DIR: ${{ github.workspace }}/ci-summary
    outputs:
      image_ref: ${{ steps.meta.outputs.image_ref }}
    steps:
      - name: Prepare summary directory
        run: mkdir -p "$SUMMARY_DIR"
      - uses: actions/checkout@v4

      - name: Read DAR_BACKUP_VERSION
        id: ver
        run: echo "v=$(cat DAR_BACKUP_VERSION)" >> $GITHUB_OUTPUT

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Set meta
        id: meta
        run: echo "image_ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.CI_TAG }}" >> $GITHUB_OUTPUT

      - name: Build with Makefile and tag locally
        run: |
          make FINAL_VERSION=dev DAR_BACKUP_VERSION=${{ steps.ver.outputs.v }} dev
          docker tag dar-backup:dev "${{ steps.meta.outputs.image_ref }}"

      - name: Push to GHCR (temp CI tag)
        run: docker push "${{ steps.meta.outputs.image_ref }}"

  test:
    needs: build
    runs-on: ubuntu-24.04
    env:
      SUMMARY_DIR: ${{ github.workspace }}/ci-summary

    steps:
      - name: Prepare summary directory
        run: mkdir -p "$SUMMARY_DIR"
      - uses: actions/checkout@v4
      - name: Set up Docker
        uses: docker/setup-buildx-action@v3

      - name: Install required system packages
        run: |
          sudo apt update
          sudo apt install -y python3

      - name: Create and prepare Python virtual environment
        run: |
          cd $GITHUB_WORKSPACE/v2
          if [[ -d venv* ]]; then
            rm -rf venv*
          fi
          python3 -m venv venv
          . venv/bin/activate
          pip install --upgrade pip
          pip install pytest pytest-json-report

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Pull CI image
        run: docker pull "${{ needs.build.outputs.image_ref }}"

      - name: Ensure image exists
        run: docker image inspect "${{ needs.build.outputs.image_ref }}" > /dev/null

      - name: Show image digest
        run: docker image inspect "${{ needs.build.outputs.image_ref }}" --format '{{.RepoTags}} {{.Id}} {{.Size}}'
      - name: Run FULL, DIFF & INCR tests
        run: |
          . venv/bin/activate
          make IMAGE="${{ needs.build.outputs.image_ref }}" test-nobuild

      - name: Summarize unit test results
        if: always()
        run: |
          if [ -f pytest-report.json ]; then
            total=$(jq '.summary.total' pytest-report.json)
            passed=$(jq '.summary.passed' pytest-report.json)
            failed=$(jq '.summary.failed' pytest-report.json)
            skipped=$(jq '.summary.skipped' pytest-report.json)
            rate=$(jq -n "100 * $passed / $total" | xargs printf "%.1f")

            jq -n --argjson total "$total" \
                  --argjson passed "$passed" \
                  --argjson failed "$failed" \
                  --argjson skipped "$skipped" \
                  --arg rate "$rate" \
                  '{
                    test: {
                      total: $total,
                      passed: $passed,
                      failed: $failed,
                      skipped: $skipped,
                      pass_rate: $rate
                    }
                  }' > "$SUMMARY_DIR/test.json"
          fi



  # Run end-to-end backup/restore/compare tests in a container
  # Run FULL, DIFF, INCR cycle
  # Create/modify/delete files between runs
  # Verify that restore matches original data after each restore
  # using a temporary dataset created in the workflow
  # This is a more realistic test of the actual backup/restore process
  # than the unit tests in `test` job. 
  backup-restore-compare:
    needs: build
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    env:
      SUMMARY_DIR: ${{ github.workspace }}/ci-summary

    steps:
      - name: Prepare summary directory
        run: mkdir -p "$SUMMARY_DIR"

      - uses: actions/checkout@v4

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Pull CI image
        run: docker pull "${{ needs.build.outputs.image_ref }}"

      - name: Ensure image exists
        run: docker image inspect "${{ needs.build.outputs.image_ref }}" > /dev/null

      - name: Read DAR_BACKUP_VERSION
        id: ver
        run: echo "v=$(cat DAR_BACKUP_VERSION)" >> $GITHUB_OUTPUT

      - name: Compute artifact naming components
        id: naming
        run: |
          DATE=$(date +%Y%m%d)
          SHA=$(sha256sum grype-report.txt | cut -c1-12)
          echo "date=$DATE" >> $GITHUB_OUTPUT
          echo "sha=$SHA" >> $GITHUB_OUTPUT

      - name: Run E2E backup/restore/compare
        run: |
          set -euo pipefail

          export WORKDIR="$(pwd)/.e2e"
          export DATA_DIR="$WORKDIR/data"
          export BACKUP_DIR="$WORKDIR/backups"
          export BACKUP_D_DIR="$WORKDIR/backup.d"
          export RESTORE_DIR="$WORKDIR/restore"
          export DATE="$(date +%Y-%m-%d)"
          echo "Using 'DATE': $DATE"
          export IMAGE="${{ needs.build.outputs.image_ref }}"

          rm -fr "$DATA_DIR" "$BACKUP_DIR" "$BACKUP_D_DIR" "$RESTORE_DIR"
          mkdir -p "$DATA_DIR" "$BACKUP_DIR" "$BACKUP_D_DIR" "$RESTORE_DIR"

          # ---------- helpers ----------
          assert_deleted() { test ! -e "$1" || { echo "expected deleted: $1"; exit 1; }; }

          assert_symlink_target() { local t; t=$(readlink "$1"); [ "$t" = "$2" ] || { echo "symlink target mismatch: $1 -> $t (want $2)"; exit 1; }; }

          assert_same_inode() { # hardlink check: same inode == preserved hardlink
            local i1 i2; i1=$(stat -c '%i' "$1"); i2=$(stat -c '%i' "$2")
            [ "$i1" = "$i2" ] || { echo "hardlink lost: $1 ($i1) vs $2 ($i2)"; exit 1; }
          }

          assert_mode() { # octal mode compare
            local m; m=$(stat -c '%a' "$1"); [ "$m" = "$2" ] || { echo "mode mismatch on $1: $m != $2"; exit 1; }; }

          add_initial_dataset () {
            mkdir -p "$DATA_DIR/sub/inner" "$DATA_DIR/with space" "$DATA_DIR/unicode-æøå"
            # small text
            printf "hello dar-backup\n" > "$DATA_DIR/hello.txt"
            # binary (256 KiB)
            head -c 262144 /dev/urandom > "$DATA_DIR/bin-256k.dat"
            # large binary (64 MiB)
            head -c $((18*1024*1024)) /dev/urandom > "$DATA_DIR/large-64m.bin"
            # nested text
            printf "nested\n" > "$DATA_DIR/sub/inner/note.md"
            # symlink (relative)
            if [ ! -L "$DATA_DIR/sub/link-to-hello" ]; then
              ln -s ../hello.txt "$DATA_DIR/sub/link-to-hello"
            fi
            # hardlink to the binary
            if [ ! -e "$DATA_DIR/bin-256k.hardlink" ]; then
              ln "$DATA_DIR/bin-256k.dat" "$DATA_DIR/bin-256k.hardlink"
            fi
            # file in dir with spaces
            echo "space dir" > "$DATA_DIR/with space/file.txt"
            # unicode file
            echo "unicode" > "$DATA_DIR/unicode-æøå/fil.txt"
          }

          add_more_data_round1 () {
            echo "round1 append" >> "$DATA_DIR/hello.txt"
            # new files
            head -c 1048576 /dev/urandom > "$DATA_DIR/new-1m.bin"
            echo "new text r1" > "$DATA_DIR/sub/new-r1.txt"
            # change target of symlink
            echo "alt target" > "$DATA_DIR/alt.txt"
            rm -f "$DATA_DIR/sub/link-to-hello"
            ln -s ../alt.txt "$DATA_DIR/sub/link-to-hello"
            # delete something to test removals
            rm -f "$DATA_DIR/sub/inner/note.md"
          }

          add_more_data_round2 () {
            echo "round2 append" >> "$DATA_DIR/hello.txt"
            head -c 2097152 /dev/urandom > "$DATA_DIR/new-2m.bin"
            mkdir -p "$DATA_DIR/newdir"
            echo "brand new r2" > "$DATA_DIR/newdir/readme.txt"
            # flip hardlink: replace with independent file
            rm -f "$DATA_DIR/bin-256k.hardlink"
            cp "$DATA_DIR/bin-256k.dat" "$DATA_DIR/bin-256k.hardlink"
          }

          list_contents () {
            archive_basename="$1"  # e.g., default_FULL_${DATE}, default_DIFF_${DATE}, default_INCR_${DATE}  
            echo "Contents of archive set: $archive_basename"
            docker run --rm   -e RUN_AS_UID=$(id -u) \
              -v "$DATA_DIR":/data \
              -v "$BACKUP_DIR":/backups \
              -v "$BACKUP_D_DIR":/backup.d \
              "$IMAGE" --list-contents  "$archive_basename" \
              --config /etc/dar-backup/dar-backup.conf    --log-stdout --verbose
          }


          restore_full () {
              archive_basename="default_FULL_${DATE}"
              list_contents "$archive_basename"
              docker run --rm   -e RUN_AS_UID=$(id -u) \
              -v "$DATA_DIR":/data \
              -v "$BACKUP_DIR":/backups \
              -v "$RESTORE_DIR":/restore \
              -v "$BACKUP_D_DIR":/backup.d \
              "$IMAGE" -r  "$archive_basename" \
              --config /etc/dar-backup/dar-backup.conf    --log-stdout --verbose
          }

          restore_diff () {
              archive_basename="default_DIFF_${DATE}"
              list_contents "$archive_basename"
              docker run --rm   -e RUN_AS_UID=$(id -u) \
              -v "$DATA_DIR":/data \
              -v "$BACKUP_DIR":/backups \
              -v "$RESTORE_DIR":/restore \
              -v "$BACKUP_D_DIR":/backup.d \
              "$IMAGE" -r  "$archive_basename" \
              --config /etc/dar-backup/dar-backup.conf   --log-stdout --verbose
          }

          restore_incr () {
              archive_basename="default_INCR_${DATE}"
              list_contents "$archive_basename"
              docker run --rm   -e RUN_AS_UID=$(id -u) \
              -v "$DATA_DIR":/data \
              -v "$BACKUP_DIR":/backups \
              -v "$RESTORE_DIR":/restore \
              -v "$BACKUP_D_DIR":/backup.d \
              "$IMAGE" -r  "$archive_basename" \
              --config /etc/dar-backup/dar-backup.conf  --log-stdout --verbose
          }

          restore_and_compare () {
            local ARCHIVE_BASENAME="$1"  # e.g., default_FULL_${DATE}, default_DIFF_${DATE}, default_INCR_${DATE}
            rm -rf "$RESTORE_DIR" && mkdir -p "$RESTORE_DIR"
            # Full restore path:
            # - For FULL: restore only the full archive
            # - For DIFF: restore full, then diff
            # - For INCR: restore full, then diff, then incr
            echo "==> Restoring set: $ARCHIVE_BASENAME"
            if [[ "$ARCHIVE_BASENAME" == "default_FULL_${DATE}" ]]; then
                echo "==> Restoring FULL only: $ARCHIVE_BASENAME"
                restore_full
            elif [[ "$ARCHIVE_BASENAME" == "default_DIFF_${DATE}" ]]; then
                echo "==> Restoring DIFF: $ARCHIVE_BASENAME"
                restore_full
                restore_diff
            elif [[ "$ARCHIVE_BASENAME" == "default_INCR_${DATE}" ]]; then
                echo "==> Restoring INCR: $ARCHIVE_BASENAME"
                restore_full
                restore_diff
                restore_incr
            else
              echo "Unknown restore set: $ARCHIVE_BASENAME" >&2
              exit 2
            fi
            echo "==> Comparing /restore vs /data for $ARCHIVE_BASENAME"
            diff -qr "$DATA_DIR" "$RESTORE_DIR/data" > "$WORKDIR/diff-${ARCHIVE_BASENAME}.txt" || true
            if [[ -s "$WORKDIR/diff-${ARCHIVE_BASENAME}.txt" ]]; then
              echo "Diff found for $ARCHIVE_BASENAME:"
              sed -n '1,120p' "$WORKDIR/diff-${ARCHIVE_BASENAME}.txt"
              echo "❌ Mismatch after restore for $ARCHIVE_BASENAME"
              exit 1
            else
              echo "✅ Restore matches source for $ARCHIVE_BASENAME"
            fi
          }
          # Create `default` backup definition in $BACKUP_D_DIR
          cat > "$BACKUP_D_DIR/default" <<EOF
          -am
          -R /
          -g data/
          -z5
          -n
          --slice 12G
          --comparison-field=ignore-owner
          --cache-directory-tagging
          EOF

          # ---------- dataset + FULL ----------
          echo "==> Creating dataset"
          add_initial_dataset

          echo "==> FULL backup"
          scripts/run-backup.sh -t FULL  
          restore_and_compare "default_FULL_${DATE}"
          restore_and_compare "default_FULL_${DATE}"
          # Hardlink must be preserved in FULL
          assert_same_inode "$RESTORE_DIR/data/bin-256k.dat" "$RESTORE_DIR/data/bin-256k.hardlink"
          # Symlink target must point to hello.txt initially
          assert_symlink_target "$RESTORE_DIR/data/sub/link-to-hello" "../hello.txt"


          # ---------- mutate + DIFF ----------
          echo "==> Add more data (R1) and DIFF backup"
          add_more_data_round1
          scripts/run-backup.sh -t DIFF 
          restore_and_compare "default_DIFF_${DATE}"
          assert_deleted "$RESTORE_DIR/data/sub/inner/note.md"
          # Symlink retarget must be applied
          assert_symlink_target "$RESTORE_DIR/data/sub/link-to-hello" "../alt.txt"

          # ---------- mutate + INCR ----------
          echo "==> Add more data (R2) and INCR backup"
          add_more_data_round2
          scripts/run-backup.sh -t INCR
          restore_and_compare "default_INCR_${DATE}"
          # We deliberately broke the hardlink in round2; ensure they are now different inodes
          DAT_INODE=$(stat -c '%i' "$RESTORE_DIR/data/bin-256k.dat")
          HARDLINK_INODE=$(stat -c '%i' "$RESTORE_DIR/data/bin-256k.hardlink")
          if [ "$DAT_INODE" = "$HARDLINK_INODE " ]; then
            echo "hardlink incorrectly preserved after round2 change"; exit 1
          fi

      - name: Summarize E2E backup/restore results
        if: always()
        run: |
          summary='{}'
          DATE="$(date +%Y-%m-%d)"
          WORKDIR="$(pwd)/.e2e"
          overall_status="success"

          for phase in FULL DIFF INCR; do
            archive="default_${phase}_${DATE}"
            diff_file="$WORKDIR/diff-${archive}.txt"

            if [[ -s "$diff_file" ]]; then
              status="mismatch"
              notes="Differences found in ${phase} restore"
              diff_lines=$(wc -l < "$diff_file")
              overall_status="failure"
            else
              status="ok"
              notes="Restore matches source"
              diff_lines=0
            fi

            summary=$(echo "$summary" | jq --arg phase "$phase" \
                                          --arg status "$status" \
                                          --argjson diff "$diff_lines" \
                                          --arg notes "$notes" \
              '. + {($phase): {status: $status, diff_lines: $diff, notes: $notes}}')
          done

          jq -n --argjson phases "$summary" \
                --arg status "$overall_status" \
                '{
                  backup_restore_compare: {
                    phases: $phases,
                    overall_status: $status
                  }
                }' > "$SUMMARY_DIR/e2e.json"




  sbom_vuln_scan:
    needs: build
    runs-on: ubuntu-latest
    timeout-minutes: 25
    env:
      GRYPE_DB_AUTO_UPDATE: "true"
    steps:
      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq
      - uses: actions/checkout@v4
      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Pull CI image
        run: docker pull "${{ needs.build.outputs.image_ref }}"

      - name: Compute Grype cache key
        id: cache-key
        run: |
          VER=$(curl -sL https://raw.githubusercontent.com/anchore/grype/main/VERSION)
          echo "ver=$VER" >> $GITHUB_OUTPUT
          echo "week=$(date +%G-%V)" >> $GITHUB_OUTPUT

      - name: Cache Grype DB
        uses: actions/cache@v4
        with:
          path: ~/.cache/grype
          key: grype-db-${{ runner.os }}-${{ runner.arch }}-${{ steps.cache-key.outputs.ver }}-${{ steps.cache-key.outputs.week }}
          restore-keys: |
            grype-db-${{ runner.os }}-${{ runner.arch }}-${{ steps.cache-key.outputs.ver }}-
            grype-db-${{ runner.os }}-${{ runner.arch }}-

      - name: Install Syft
        run: curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
      - name: Install Grype
        run: curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin

      - name: Update Grype DB (optional)
        if: env.GRYPE_DB_AUTO_UPDATE == 'true'
        run: grype db update

      - name: Show Grype DB status
        run: grype db status

      - name: Generate SBOM
        run: syft "docker:${{ needs.build.outputs.image_ref }}" -o cyclonedx-json > sbom-cyclonedx.json

      - name: SBOM sanity check
        run: |
          test -s sbom-cyclonedx.json
          wc -c sbom-cyclonedx.json
          grep -q '"components"' sbom-cyclonedx.json || { echo 'SBOM missing components key'; exit 1; }

      - name: Scan with Grype (fail on high/critical)
        run: |
          set -e
          grype "sbom:sbom-cyclonedx.json" -o table --fail-on High | tee grype-report.txt
          grype "sbom:sbom-cyclonedx.json" -o sarif > grype.sarif

      - name: Upload SBOM
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sbom-cyclonedx
          path: sbom-cyclonedx.json

      - name: Upload Grype report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: grype-report
          path: grype-report.txt

      - name: Upload SARIF
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: grype.sarif

      - name: Job summary
        if: always()
        run: |
          comps=$(jq '.components|length' sbom-cyclonedx.json || echo 0)
          vulns=$(grype "sbom:sbom-cyclonedx.json" -o json | jq '.matches|length' || echo 0)
          {
            echo "### SBOM / Vulnerabilities"
            echo "- Image: ${{ needs.build.outputs.image_ref }}"
            echo "- Components: $comps"
            echo "- Vulnerabilities (all severities): $vulns"
          } >> "$GITHUB_STEP_SUMMARY"


      - name: Summarize SBOM and vulnerabilities by severity
        if: always()
        run: |
          comps=$(jq '.components|length' sbom-cyclonedx.json || echo 0)

          grype "sbom:sbom-cyclonedx.json" -o json > grype-results.json

          total=$(jq '.matches | length' grype-results.json)
          critical=$(jq '[.matches[] | select(.vulnerability.severity == "Critical")] | length' grype-results.json)
          high=$(jq '[.matches[] | select(.vulnerability.severity == "High")] | length' grype-results.json)
          medium=$(jq '[.matches[] | select(.vulnerability.severity == "Medium")] | length' grype-results.json)
          low=$(jq '[.matches[] | select(.vulnerability.severity == "Low")] | length' grype-results.json)
          negligible=$(jq '[.matches[] | select(.vulnerability.severity == "Negligible")] | length' grype-results.json)
          unknown=$(jq '[.matches[] | select(.vulnerability.severity == "Unknown")] | length' grype-results.json)

          jq -n --arg image "${{ needs.build.outputs.image_ref }}" \
                --argjson comps "$comps" \
                --argjson total "$total" \
                --argjson critical "$critical" \
                --argjson high "$high" \
                --argjson medium "$medium" \
                --argjson low "$low" \
                --argjson negligible "$negligible" \
                --argjson unknown "$unknown" \
                '{
                  sbom_vuln_scan: {
                    image: $image,
                    components: $comps,
                    vulnerabilities: {
                      total: $total,
                      by_severity: {
                        critical: $critical,
                        high: $high,
                        medium: $medium,
                        low: $low,
                        negligible: $negligible,
                        unknown: $unknown
                      }
                    }
                  }
                }' > "$SUMMARY_DIR/sbom.json"

  summarize:
    runs-on: ubuntu-24.04
    needs: [test, backup-restore-compare, sbom_vuln_scan]

    env:
      SUMMARY_DIR: ${{ github.workspace }}/ci-summary

    steps:
      - name: Prepare summary directory
        run: mkdir -p "$SUMMARY_DIR"

      - uses: actions/checkout@v4

      - name: Merge summaries
        run: |
          mkdir -p ci-summary
          jq -s 'reduce .[] as $item ({}; . * $item)' ci-summary/*.json > ci-summary/ci-summary.json
          cat ci-summary/ci-summary.json  # print to Workflow log

      - name: Upload combined summary
        uses: actions/upload-artifact@v4
        with:
          name: ci-summary
          path: ci-summary/ci-summary.json


      - name: Create metadata-wrapped summary
        run: |
          mkdir -p data
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          jq -n \
            --arg ts "$timestamp" \
            --arg workflow "${{ github.workflow }}" \
            --arg commit "${{ github.sha }}" \
            --arg branch "${{ github.ref }}" \
            --arg actor "${{ github.actor }}" \
            --arg run_id "${{ github.run_id }}" \
            --arg run_url "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            --slurpfile summary ci-summary/ci-summary.json \
            '{
              timestamp: $ts,
              workflow_name: $workflow,
              git_commit: $commit,
              branch: $branch,
              actor: $actor,
              run_id: $run_id,
              run_url: $run_url,
              summary: $summary[0]
            }' > data/new-entry.json



      - name: Append to build-test-scan-report.json
        run: |
          if [ ! -f data/build-test-scan-report.json ]; then
            echo "[]" > data/build-test-scan-report.json
          fi

          jq -s '.[0] + [.[1]]' \
            data/build-test-scan-report.json data/new-entry.json > data/tmp.json && \
          mv data/tmp.json data/build-test-scan-report.json



      - name: Commit and push updated build-test-scan-report.json
        if: github.ref == 'refs/heads/main'
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"

          git fetch origin
          git checkout main

          # Check if the file has changed before staging
          if ! git diff --quiet data/build-test-scan-report.json; then
            git add data/build-test-scan-report.json
            git commit -m "Update build-test-scan-report.json with latest CI summary"
            git push origin main
          else
            echo "No changes detected in build-test-scan-report.json. Skipping commit."
          fi

